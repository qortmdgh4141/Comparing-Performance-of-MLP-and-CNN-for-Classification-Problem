# ğŸ“‰  Comparing Performance of MLP and CNN for Classification Problem
<br/>
  
### 1. &nbsp; Research Objective <br/><br/>

- _The objective of this research is to train MLP-based models and CNN-based models using the Fashion-MNIST dataset and compare the classification performance of these two models. The main hypothesis of this study is as follows:_  <br/>

  - _"The CNN-based model will show superior performance in the classification task of fashion item images compared to the MLP-based model."_ <br/><br/>

- _This hypothesis is based on the following prior background knowledge:_  <br/>

  - _Fashion-MNIST is a dataset consisting of fashion item images, which include various patterns and textures. To handle such complex visual features, it is expected that CNN models, which can effectively utilize spatial information, would be more suitable._ <br/>
  
  - _MLP is an artificial neural network with fully connected hidden layers between input and output. This model is suitable for sequential data processing, but it may have difficulty effectively extracting visual features of fashion item images._ <br/>
  
  - _CNN is a deep learning model specialized in image processing, which handles local information through convolution and pooling operations using shared parameters. Such a structure can better capture the visual features of fashion item images and be more suitable for classification tasks._ <br/><br/>
  
- _Based on this background, we will train MLP-based models and CNN-based models, respectively, and apply them to the classification task of the Fashion-MNIST dataset. It is expected that these results will contribute to the improvement of performance in image classification tasks and aid in model selection in practical applications._ <br/><br/><br/> 

### 2. &nbsp; Key Components of the Neural Network Model and Experimental Settings  <br/><br/>

- _Convolutional Layer_<br/>

  - _Number of filters : (32 or 64)  /  Kernel size: (3 x 3)._ <br/>
  
  - _The convolutional layer extracts local features using filters (kernels) and generates feature maps._ <br/><br/>

- _Pooling Layer_<br/>

  - _Pooling size : (2, 2)._<br/>
  
  - _The pooling layer provides spatial invariance, reduces the size of feature maps to decrease computational complexity, and emphasizes abstracted features._ <br/><br/>

- _Dense Layer_ <br/>

  - _Number of nodes: (512 or 10)._ <br/>

  - _The dense layer is a traditional neural network layer that connects all inputs and outputs. It learns abstract features and outputs probability distribution for various classes._<br/><br/>
  
- _Dropout Layer_ <br/>

  - _The dropout layer is one of the regularization techniques used to reduce overfitting during the neural network training process._ <br/>

  - _Dropout randomly deactivates some units (neurons) of the neural network during training, preventing the model from relying too heavily on specific units and improving generalization capability._<br/><br/>

- _Activation function for hidden layers : ReLU Function_ <br/>

  - _The ReLU function is a non-linear function that outputs 0 for negative input values and keeps the output as is for positive input values._ <br/>

  - _To alleviate the issue of gradient vanishing caused by weight initialization when using ReLU activation function, the weights of the hidden layers were initialized using He initialization._<br/><br/>

- _Activation function for the output layer : Softmax Function_ <br/>

  - _The softmax function is commonly used as the activation function for the output layer in multi-class classification problems._ <br/>

  - _The softmax function normalizes the input values to calculate the probability of belonging to each class, and the sum of probabilities for all classes is 1._<br/><br/>

- _Optimization Algorithm : Adam (Adaptive Moment Estimation)_ <br/>

  - _The Adam optimization algorithm, which combines the advantages of Momentum, which adjusts the learning rate considering the direction of gradients, and RMSProp, which adjusts the learning rate considering the magnitude of gradients, was used._ <br/>

  - _The softmax function normalizes the input values to calculate the probability of belonging to each class, and the sum of probabilities for all classes is 1._<br/><br/>

- _Loss Function : Cross-Entropy Loss Function_ <br/>

  - _When using the softmax function in the output layer, the cross-entropy loss function is commonly used as the loss function._ <br/>

  - _The cross-entropy loss function calculates the error only for the classes corresponding to the actual target values and updates the model in the direction of minimizing the error._<br/><br/>

- _Evaluation Metric : Accuracy_ <br/>

  - _Accuracy is one of the evaluation metrics used to assess the performance of a classification model._ <br/>

  - _Accuracy considers the prediction as correct if it matches the actual target class and calculates it by dividing it by the total number of samples._<br/><br/>

- _Batch Size & Maximum Number of Learning Iterations_ <br/>

  - _In this experiment, the batch size is 128, and the model is trained by iterating up to a maximum of 100 times._<br/>
  
  - _The number of batch size and iterations during training affects the speed and accuracy of the model, and I, as the researcher conducting the experiment, have set the number of batch size and iterations based on my experience of tuning deep learning models._<br/><br/> <br/> 

### 3. &nbsp; Data Preprocessing and Analysis <br/><br/>

- _**Package Settings**_ <br/> 
  
  ```
  from sklearn.datasets import load_diabetes
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler, StandardScaler
  from sklearn.metrics import mean_absolute_percentage_error

  from keras import initializers
  from keras.optimizers import Adam
  from keras.models import Sequential
  from keras.layers import Dense, Dropout,BatchNormalization

  import numpy as np
  import pandas as pd
  import seaborn as sns
  import matplotlib.pyplot as plt
  from IPython.display import display
  ```

- _**Data Preparation**_ <br/> 
  
  ```
  # ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ ë¡œë”© : ì…ë ¥ ë°ì´í„°(data), ëª©í‘œ ë°ì´í„°(target)
  diabetes = load_diabetes()

  # ì…ë ¥ ë°ì´í„°ì™€ ëª©í‘œ ë°ì´í„°ë¥¼ ê°ê° ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
  x_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
  y_data = pd.DataFrame(diabetes.target, columns=['target'])

  # ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì— NaNê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
  if x_data.isnull().values.any() or y_data.isnull().values.any():
      print("- ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì—ëŠ” NaNê°’ì´ ì¡´ì¬í•©ë‹ˆë‹¤. -", end= "\n\n")
  else:
      print("- ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì—ëŠ” NaNê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. -", end= "\n\n")
  ```
  
  ```
  # ì…ë ¥ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"< ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì„± : {x_data.shape[0]}í–‰ x {x_data.shape[1]}ì—´ >")
  display(x_data)
  ```
  
  ```
  # ë¶„ì„ ëŒ€ìƒì—ì„œ ì œì™¸í•  ë³€ìˆ˜ì¸ "age & sex" ì—´ì„ ì‚­ì œ  
  x_data = x_data.drop(['age', 'sex'], axis=1) 

  #  ë¶„ì„ ëŒ€ìƒì—ì„œ ì œì™¸í•  ë³€ìˆ˜ì¸ "age & sex" ì—´ì„ ì‚­ì œí•œ ì…ë ¥ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"\n< 'age & sex' ì—´ì„ ì‚­ì œí•œ ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì„± : {x_data.shape[0]}í–‰ x {x_data.shape[1]}ì—´ >")
  display(x_data)
  ```
  
  ```
  # ëª©í‘œ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"\n< ëª©í‘œ ë°ì´í„°ì˜ êµ¬ì„± : {y_data.shape[0]}í–‰ x {y_data.shape[1]}ì—´ >")
  display(y_data)
  ```
  
- _**Exploratory Data Analysis (EDA)**_ <br/> 
  
  ```
  # ê·¸ë˜í”„ë¡œ ë°ì´í„° ë¶„í¬ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì…ì¶œë ¥ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í…Œì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©
  concat_data = pd.concat([x_data, y_data], axis=1)
  display(concat_data)
  ```

  ```
  # ëª©í‘œë³€ìˆ˜ì¸ ë‹¹ë‡¨ë³‘ ì§„í–‰ ìƒíƒœ(Diabetes Progression) ê°’ì„ 10ê°œì˜ ê³„ê¸‰ìœ¼ë¡œ í•˜ëŠ” ë°€ë„ê·¸ë˜í”„ë¥¼ ì¶œë ¥
  # í‰ê· ì ìœ¼ë¡œ ë‹¹ë‡¨ë³‘ ì§„í–‰ ìƒíƒœ(Diabetes Progression) ê°’ì€ 100ì— ë§ì´ ë¶„í¬ 
  sns.set(rc={'figure.figsize' : (15, 3)})
  sns.kdeplot(data=concat_data, x='target', shade=True)
  plt.xlabel('Diabetes Progression')
  plt.show()
  ```
  
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/density_graph.png?raw=true" height="320">
  
  ```
  # ê° ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ë¥¼ íˆíŠ¸ë§µ ê·¸ë˜í”„ë¡œ ì¶œë ¥ 
  # s1 ë³€ìˆ˜ì™€ s2 ë³€ìˆ˜ë“¤ì€ ì–‘ì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŒ
  # s3 ë³€ìˆ˜ì™€ s4 ë³€ìˆ˜ë“¤ì€ ìŒì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŒ
  corr_matrix = concat_data.corr().round(2)

  sns.set(rc={'figure.figsize' : (8, 5)})
  sns.heatmap(data=corr_matrix, xticklabels=True, annot=True)
  plt.xticks(rotation=0)
  plt.xlabel('\n< Correlation coefficient between each variable >')
  plt.show()
  ```
  
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/corr_heatmap_graph.png?raw=true" width="640">
  
  ```
  # ë…ë¦½ ë³€ìˆ˜ ê°„ ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°, ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œê°€ ë°œìƒí•¨ 
  # ë”°ë¼ì„œ ë³€ìˆ˜ ì„ íƒ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë¥¼ ì œê±°
  x_data = x_data.drop(['s2','s3'], axis=1) 
  display(x_data)
  ``` 
  
- _**Splitting Data**_ <br/>  
  
  ```
  # í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¥¼ 7:3ìœ¼ë¡œ ë¶„ë¦¬
  x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=20183047)
  x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=20183047)

  print(f"- í•™ìŠµìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_train.shape}")
  print(f"- í•™ìŠµìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_train.shape}", end="\n\n")
  print(f"- ê²€ì¦ìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_val.shape}")
  print(f"- ê²€ì¦ìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_val.shape}", end="\n\n") 
  print(f"- í‰ê°€ìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_test.shape}")
  print(f"- í‰ê°€ìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_test.shape}"))   
  ```
  
- _**Feature Scaling**_ <br/> 
  
  ```
  # ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ ë˜ë„ë¡ ë°ì´í„°ì— ëŒ€í•´ ì •ê·œí™”
  # ìµœì†Œ-ìµœëŒ€ ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
  minmax_scalerX = MinMaxScaler()
  minmax_scalerY = MinMaxScaler()

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµìš© ë°ì´í„°ì— ë§ì¶¤
  minmax_scalerX.fit(x_train)
  minmax_scalerY.fit(y_train)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë³€í™˜
  x_train_minmax = minmax_scalerX.transform(x_train)
  y_train_minmax = minmax_scalerY.transform(y_train)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ê²€ì¦ìš© ë°ì´í„°ë¥¼ ë³€í™˜
  x_val_minmax = minmax_scalerX.transform(x_val)
  y_val_minmax = minmax_scalerY.transform(y_val)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë³€í™˜
  x_test_minmax = minmax_scalerX.transform(x_test)
  y_test_minmax = minmax_scalerY.transform(y_test)
  ```
  
  ```
  # ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ ë˜ë„ë¡ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ì •ê·œí™”
  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í•™ìŠµ ë°ì´í„°ì˜ ì…ë ¥ ê°’
  scalerX = MinMaxScaler()
  scalerX.fit(x_train)
  x_train_norm = scalerX.transform(x_train)

  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í•™ìŠµ ë°ì´í„°ì˜ ì¶œë ¥ ê°’
  scalerY = MinMaxScaler()
  scalerY.fit(y_train)
  y_train_norm = scalerY.transform(y_train)

  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì…ì¶œë ¥ ê°’
  x_test_norm = scalerX.transform(x_test)
  y_test_norm = scalerY.transform(y_test)
  ```
  <br/> 

### 4. &nbsp; Training and Testing MLP Model <br/><br/>

- _Optimized MLP Model_

  ```
  """
  1. ì…ì¶œë ¥ ë…¸ë“œ : 6ê°œ / 1ê°œ
     - í•™ìŠµ ì‹œì— ì…ë ¥ ë³€ìˆ˜ì˜ íŠ¹ì„± ê°¯ìˆ˜ê°€ 8ê°œì´ê³ , ëª©í‘œ ë³€ìˆ˜ ê°¯ìˆ˜ê°€ 1ê°œì´ê¸° ë•Œë¬¸ì—, ê·¸ì— ëŒ€ì‘í•˜ëŠ” ì…ì¶œë ¥ ë…¸ë“œë¡œ êµ¬ì„±

  2. ì€ë‹‰ì¸µ ê°œìˆ˜ (ë…¸ë“œ ìˆ˜) : 3ê°œ (60, 120, 60)
      - ì´ 3ê°œì˜ ì€ë‹‰ì¸µì´ ì¡´ì¬í•˜ë©°, ì œ 1 ì€ë‹‰ì¸µê³¼ ì œ 3 ì€ë‹‰ì¸µì€ 6ê°œì˜ ë…¸ë“œê°€ ì¡´ì¬í•˜ê³  ì œ 2 ì€ë‹‰ì¸µì—ëŠ” 12ê°œì˜ ë…¸ë“œê°€ ì¡´ì¬

  3. ë°°ì¹˜ ì •ê·œí™”
      - ê° ì¸µ(layer)ì„ ê±°ì¹  ë•Œë§ˆë‹¤ ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë³€í™”í•¨ì— ë”°ë¼ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ëŠ” ë¬¸ì œì¸ ë‚´ë¶€ ê³µë³€ëŸ‰(internal covariate shift)ë¥¼ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©
      - ê° ì¸µì—ì„œ ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³ , í•™ìŠµ ì¤‘ì— ì´ì— ëŒ€í•œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì ˆí•˜ì—¬ ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì•ˆì •í™” ê°€ëŠ¥

  4. í™œì„±í™” í•¨ìˆ˜ :  Relu
     - ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ê²½ìš°ëŠ” 0ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , 0ë³´ë‹¤ í° ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì¸ Relu í•¨ìˆ˜ë¡œ ì„¤ì •
     - ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” He ì´ˆê¹ƒê°’ì„ ì‚¬ìš©

  5. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ 
     - Momentumê³¼ RMSPropì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì¸ Adam(Adaptive Moment Estimation)ì„ ì‚¬ìš©
     - Momentumì€ : ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ 
     - RMSProp : ê¸°ìš¸ê¸° í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ

  6. ì†ì‹¤ í•¨ìˆ˜ : 
     - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•œ ê°’ì˜ í‰ê· ì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨, 
       ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” MSE(Mean Squared Error)ë¥¼ ì‚¬ìš©

  7. ì •í™•ë„ í‰ê°€ ì§€í‘œ
     - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ë°±ë¶„ìœ¨ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì„ í‰ê· í•˜ëŠ” MAPE(Mean Absolute Percentage Error)ë¥¼ ì‚¬ìš©
       íšŒê·€ë¶„ì„ì—ì„œ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‰ê°€ì§€í‘œ ì¤‘ ìƒëŒ€ì ì¸ ì˜¤ì°¨ì˜ í¬ê¸°ë¥¼ í‰ê°€í•˜ë¯€ë¡œ, 
       ì´ í‰ê°€ì§€í‘œì˜ ì˜¤ì°¨ ê°’ì€ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ í´ìˆ˜ë¡ ë” ì»¤ì§€ëŠ” ê²½í–¥ì´ ìˆìŒ

  8. ë°°ì¹˜ ì‚¬ì´ì¦ˆ / ìµœëŒ€ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ : 64 / 1000
  """

  # ëª¨í˜• êµ¬ì¡°
  model = Sequential()

  model.add(Dense(60, input_dim=6, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(120, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(60, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(1, bias_initializer=initializers.Constant(value=0.01)))

  model.compile(optimizer=Adam(lr=0.0001), loss='mse')

  results_standard = model.fit(x_train_minmax, y_train_minmax, validation_data=(x_val_minmax, y_val_minmax)
              , epochs=1000, batch_size=64)
  ```

  ```
  # MAPE ê°’ ì¶œë ¥ 
  y_pred = model.predict(x_test_minmax)
  y_pred_inverse = minmax_scalerY.inverse_transform(y_pred)

  minmax_mape = mean_absolute_percentage_error(y_test, y_pred_inverse)
  print("MAPE based on min-max normalization : {:.2%}".format(minmax_mape))
  ```
  <br/> 
  
### 5. &nbsp; Research Results  <br/><br/>
    
- _The purpose of this study was to train and evaluate a multilayer perceptron model on the Diabetes 130-US hospitals for years 1999-2008 Data Set. I set the number of hidden racers by referring to the optimal model structure proposed in the paper "Diabetes Mellitus Diagnosis Using Artificial Neural Networks with Fewer Features", increased the model complexity by making the number of nodes in each layer about 10 times larger than the number of nodes set in the paper, and added batch regularization and dropout layers to solve the problems of internal covariate shift and overfitting. In addition, I improved the performance of the existing model by using variable selection techniques, scaling techniques, and initialization techniques to solve multicollinearity problems and to prevent problems such as gradients vanishing._ <br/> <br/>

  ```
  # loss ê·¸ë˜í”„ ì¶œë ¥
  train_loss = results_standard.history['loss']
  val_loss = results_standard.history['val_loss']

  epochs = range(1, len(train_loss) + 1)

  plt.plot(epochs, train_loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.ylim([0,4])
  plt.legend()
  plt.show()
  ```
  
  ```
  # ì˜ˆì¸¡ê°’ ëŒ€ë¹„ ì‹¤ê²Œê°’ì˜ ì‚°í¬ë„
  y_pred = model.predict(x_test_minmax)
  diff = np.abs(y_pred - y_test_minmax)

  plt.figure(figsize=(5, 5))
  plt.scatter(y_test_minmax, y_pred, c=diff, cmap='viridis')
  plt.plot([0, 1], [0, 1], c='r')
  plt.xlabel('True Values')
  plt.ylabel('Predictions')
  plt.colorbar()
  plt.show()
  ```
  <br/>
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/scatter_plot_line_graph.png?raw=true" weight="1280">

- _The following graph shows the evolution of the loss with increasing epoch, and I can see that overfitting did not occur due to the impact of the aforementioned batch regularization and dropout layer._ <br/>

- _However, it was found that underfitting occurs, where the predictive performance on the training data does not improve because it does not sufficiently reflect the complexity of the data after some learning progress._ <br/>

- _This means that the model has been oversimplified, and I believe that the following reasons contributed to this:_ <br/>

  - _Low Model Complexity_<br/>
  
    - _I believe that underfitting occurs because the model is too simple or limited._<br/>
    
    - _This does not mean that making the current model structure more complex is a good solution. This is because the amount of training data is currently small, and making the model structure more complex is very likely to lead to overfitting._ <br/><br/>
    
  - _Lack of Variable Diversity_<br/>
  
    - _It is determined that underfitting occurred due to a lack of variable diversity in the dataset._<br/>
    
    - _To be more specific, I believe that excessive normalization was applied to a dataset that lacks diversity, causing the model to fail to adequately reflect the patterns in the training data._<br/> <br/> <br/>

### 6. &nbsp; Suggestions for Future Research  <br/><br/>
    
- _Based on the scatter plot of predicted values compared to actual values, it can be seen that the performance of the prediction model is not very accurate. Therefore, to improve the performance of the regression model in the future, the following solutions can be suggested:_

  - _Diversity and Quality of Data_<br/>
  
    - _The unbalanced distribution of data can greatly affect the performance of the model.Therefore, if the data is collected considering the diversity of the data and preprocessed so as not to harm the diversity of the collected data set, the performance of the model will be improved._<br/>

  - _Feature Engineering_<br/>
  
    - _Feature engineering is known to have a significant impact on the performance of a prediction model.Therefore, by extracting more diverse and sophisticated features or introducing new variables, it is expected to improve the performance of the model._<br/> <br/> <br/>
 
--------------------------
### ğŸ’» S/W Development Environment
<p>
  <img src="https://img.shields.io/badge/Windows 10-0078D6?style=flat-square&logo=Windows&logoColor=white"/>
  <img src="https://img.shields.io/badge/Google Colab-black?style=flat-square&logo=Google Colab&logoColor=yellow"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
</p>
<p>
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white"/>
  <img src="https://img.shields.io/badge/scikit learn-blue?style=flat-square&logo=scikitlearn&logoColor=F7931E"/>
  <img src="https://img.shields.io/badge/Numpy-013243?style=flat-square&logo=Numpy&logoColor=blue"/>
</p>

### ğŸš€ Machine Learning Model
<p>
  <img src="https://img.shields.io/badge/MLP-5C5543?style=flat-square?"/>
</p> 

### ğŸ’¾ Datasets used in the project
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Diabetes Dataset <br/>
